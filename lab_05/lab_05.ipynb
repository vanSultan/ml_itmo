{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.\tDownload Alice in Wonderland by Lewis Carroll from Project Gutenberg's website \n",
    "    http://www.gutenberg.org/files/11/11-0.txt\n",
    "2.\tPerform any necessary preprocessing on the text, including converting to lower case, \n",
    "    removing stop words, numbers / non-alphabetic characters, lemmatization.\n",
    "3.\tFind Top 10 most important (for example, in terms of TF-IDF metric) words \n",
    "    from each chapter in the text (not \"Alice\"); how would you name each chapter \n",
    "    according to the identified tokens?\n",
    "4.\tFind the Top 10 most used verbs in sentences with Alice. What does Alice do most often?\n",
    "5.\t*(not necessary) Find Top 100 most used verbs in sentences with Alice. \n",
    "    Get word vectors using a pre-trained word2vec model and visualize them. \n",
    "    Compare the words using embeddings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from os import listdir"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sultan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sultan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sultan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Sultan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load text into memory\n",
    "with open(\"./11-0.txt\", \"r\", encoding=\"utf8\") as f_src:\n",
    "    text = f_src.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(txt):\n",
    "    tokens = txt.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens = [word for word in tokens if len(word) > 3]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Turn a text into clean tokens\n",
    "words = clean_text(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lem_words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "lem_words = [word.lower() for word in lem_words] \n",
    "print(lem_words[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Chapters\n",
    "chapters, tmp_chap = list(), list()\n",
    "for word in lem_words:\n",
    "    if word == \"chapter\":\n",
    "        tmp_chap = list() \n",
    "        chapters.append(tmp_chap)\n",
    "    else:\n",
    "        tmp_chap.append(word)\n",
    "\n",
    "print(f\"Total count of chapters: {len(chapters)}\")\n",
    "\n",
    "print(\"First 3 words per chapter:\")    \n",
    "for i in range(len(chapters)):\n",
    "    print(chapters[i][0:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TF\n",
    "def get_tf(word, f_text):\n",
    "    tf_text = Counter(f_text)\n",
    "    tf_text[word] = tf_text[word] / float(len(f_text))\n",
    "        \n",
    "    return tf_text[word]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IDF\n",
    "def get_idf(word, chaps):\n",
    "    count = sum([1.0 for chap in chaps if word in chap])\n",
    "    if count != 0:\n",
    "        return math.log(len(chaps) / count)\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Note\n",
    "\n",
    "I would name them by the first three words found."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Top-10 TF-IDF in chapters\n",
    "chapters_list = list()\n",
    "for chapter in chapters:\n",
    "    vocab_chapters = Counter()\n",
    "    for word in chapter:\n",
    "        if word != \"alice\":\n",
    "            vocab_chapters[word] = get_tf(word, chapter) * get_idf(word, chapters)\n",
    "            vocab_chapters[word] = round(vocab_chapters[word], 5)\n",
    "    chapters_list.append(vocab_chapters)\n",
    "        \n",
    "for chapter in chapters_list:\n",
    "    print(\"Chapter \" + str(chapters_list.index(chapter) + 1))\n",
    "    print(pd.DataFrame(chapter.most_common(10)))\n",
    "    print()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Note\n",
    "\n",
    "Alice most often says something, goes somewhere and thinks about something."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Verbs with 'Alice'\n",
    "vocab_verbs = Counter()\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence_words = clean_text(sentence)\n",
    "    sentence_words = [wordnet_lemmatizer.lemmatize(w, pos='v') for w in sentence_words]\n",
    "    sentence_words = [w.lower() for w in sentence_words]\n",
    "    if \"alice\" in sentence_words:\n",
    "        sentence_words = nltk.pos_tag(sentence_words)\n",
    "        for word, pos_tag in sentence_words:\n",
    "            if pos_tag == \"VB\":\n",
    "                vocab_verbs[word] += 1 \n",
    "pd.DataFrame(data=vocab_verbs.most_common(10), columns=[\"Verb\", \"Count\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}